{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_89/878238727.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the downsides of the basic image processing strategy is that is requires that you hand designed rules for detection, which limits you to relatively simple choices and will reduce performance. A machine learning approach is to learn these filters (or rules) from example training data. We'll briefly introduce a deep learning strategy to do this here. We'll use [pytorch](https://pytorch.org/tutorials/beginner/basics/intro.html), a deep learning software library to train a simple neural network that can predict a target object location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some training/ test data\n",
    "\n",
    "In practice, the hardest part of supervised machine learning is dataset collection and cleaning. If you use this strategy in your project, you will need to label many images with target object positions. Our toy datatset will consist of randomly placed red squares on a noisy background. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate some data\n",
    "\n",
    "N = 1000\n",
    "X = (100*np.random.rand(N,64,64,3)).astype(int) # block images\n",
    "\n",
    "Y = np.random.randint(5,59,size=(N,2)) # block positions\n",
    "\n",
    "for j in range(N):\n",
    "    X[j,Y[j,1]-5:Y[j,1]+5,Y[j,0]-5:Y[j,0]+5,0] = 250\n",
    "    X[j,Y[j,1]-5:Y[j,1]+5,Y[j,0]-5:Y[j,0]+5,1] = 50\n",
    "    X[j,Y[j,1]-5:Y[j,1]+5,Y[j,0]-5:Y[j,0]+5,2] = 50\n",
    "    \n",
    "plt.figure(figsize=(15,5))\n",
    "for j in range(4):\n",
    "    plt.subplot(1,4,j+1)\n",
    "    plt.imshow(X[j,:,:,:],extent=[0,64,64,0])\n",
    "    plt.plot(Y[j,0],Y[j,1],'bo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset consists of images, stored in the variable X, and corresponding 2D object positions, stored in the variable Y. We'll split our data into two, one set for training, and keep another set aside for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = X[0:500,:,:,:]\n",
    "Ytrain = Y[0:500,:]\n",
    "\n",
    "Xtest = X[500:,:,:,:]\n",
    "Ytest = Y[500:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a simple CNN using PyTorch\n",
    "\n",
    "We'll now build a very basic convolutional neural network for regression. Our network will consist of 3 convolution layers (filters with learnable parameters) and 2 fully connected layers. We'll use relu activations to introduce non-linearities into the network, and use a final tanh activation to make sure our network predicts values between -1 and 1. We'll also scale our training data to follow this constraint too.\n",
    "\n",
    "NB: This is a basic model for demonstration purposes - it should work on very simple datasets, and might work on the type of images you'll get in the arena, but there are much better models out there. Check out [YOLO](https://pjreddie.com/darknet/yolo/) and think about looking into fine tuning a pre-trained model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "# The Detector class will be our detection model\n",
    "class Detector(nn.Module):\n",
    "\n",
    "    def __init__(self,output_dim,image_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 3, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(3, 3, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(3, 3, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            Flatten(),\n",
    "            nn.Linear(192,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        positions = self.encoder(x)\n",
    "        \n",
    "        return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll declare our network\n",
    "network = Detector(output_dim=2,image_channels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our model\n",
    "\n",
    "Ok, so we've defined our network, now we will pass our training data through this network in manageable batches, compute the prediction error, and adjust the network parameters to reduce this error. We'll use the Adam optimizer to do this, and repeat a number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nepochs = 100\n",
    "Nbatch = 8\n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(),lr=1e-4)\n",
    "\n",
    "losses = []\n",
    "for j in range(Nepochs):\n",
    "    \n",
    "    # Shuffle our training data after each epoch\n",
    "    idxs = np.arange(Xtrain.shape[0])\n",
    "    np.random.shuffle(idxs)\n",
    "    \n",
    "    Xtrain = Xtrain[idxs,:,:,:]\n",
    "    Ytrain = Ytrain[idxs,:]\n",
    "    \n",
    "    # Loop over training data in batches of images\n",
    "    batch_losses = []\n",
    "    for k in range(int(Xtrain.shape[0]/Nbatch)):\n",
    "        \n",
    "        # Scale images and positions to be between 0 and 1 and convert to tensors for pytorch\n",
    "        Xbatch = torch.from_numpy(Xtrain[Nbatch*k:Nbatch*(k+1),:,:,:]).float().transpose(1,3)/255\n",
    "        Ybatch = torch.from_numpy(Ytrain[Nbatch*k:Nbatch*(k+1),:]).float()/64-0.5\n",
    "        \n",
    "        # Predict positions using neural network\n",
    "        Ypred = network(Xbatch)\n",
    "        \n",
    "        # Calulate the loss (error between predictions and true values)\n",
    "        loss = torch.sum((Ypred-Ybatch)**2)\n",
    "        \n",
    "        # Zero existing gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Adjust neural network weights and biases by taken a step in a direction that reduces the loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_losses.append(loss.item())\n",
    "        \n",
    "    losses.append(np.mean(batch_losses))\n",
    "    \n",
    "    \n",
    "    plt.clf()\n",
    "    plt.plot(losses)\n",
    "    plt.ylabel('Loss (mean squared error)')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.grid()\n",
    "\n",
    "    \n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our model\n",
    "\n",
    "Ok, so our average error is pretty low, lets test on sample random images from our test set. We'll just do this as a sanity check, if you want to be sure, it's a good idea to test on the whole test set and compute some error metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.random.randint(Xtest.shape[0]/Nbatch)\n",
    "test_ims = torch.from_numpy(Xtest[Nbatch*k:Nbatch*(k+1),:,:,:]).float().transpose(1,3)/255\n",
    "Ypred = network(test_ims).detach().numpy()\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "for j in range(Nbatch):\n",
    "    plt.subplot(1,Nbatch,j+1)\n",
    "    plt.imshow(Xtest[Nbatch*k+j,:,:,:].squeeze(),extent=[0,64,64,0])\n",
    "    \n",
    "    pos_pred = (Ypred+0.5)*64 # Scale predictions to pixel coordinates\n",
    "    \n",
    "    # Show predicted marker positions in image\n",
    "    plt.plot([pos_pred[j,0]],[pos_pred[j,1]],'bo',markersize=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once trained, you can save a model and then load this to re-use in another program. See [this link](https://pytorch.org/tutorials/beginner/saving_loading_models.html) for details. In typical use, you'll pass each image through this network as they are received, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Xtest[100,:,:,:].reshape(1,64,64,3)\n",
    "torch_im = torch.from_numpy(im).transpose(1,3)/255.0 # Convert to torch tensor, reshape and normalise\n",
    "torch_pos = network(torch_im) # Feed through network\n",
    "\n",
    "pos = (torch_pos.detach().numpy()+0.5)*64 #Convert back to numpy\n",
    "\n",
    "print('Detected position: ',pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, we have a simple object recognition strategy. You may find that lighting and motion blur makes this ineffective through, and you need a lot of data to make this work. You'll also find that your robot moves in a 3D world, but this is a 2D detection in image space. Mapping between these spaces can be difficult. If you know the geometry of the object and properties of your camera, it is possible to infer the 3D location of the object. (See https://docs.opencv.org/master/d9/d0c/group__calib3d.html)\n",
    "\n",
    "But if you don't know this, the best option might be to do visual servoing - rotating the robot so that the target object moves to a desired position in the image. You could try to adopt a proportional control approach, like the one we used previously, but acting on the image error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
